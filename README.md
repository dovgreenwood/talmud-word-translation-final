## About the Project
This project used computational methods to connect every word in the Babylonian Talmud with its proper entry in the Jastrow Talmudic dictionary. I.e. it is a single-word translation feature, a la the Google Translate Chrome extension. Development has now concluded, and the dataset will be used by [TalmudLab](https://dilac.iac.gatech.edu/node/66) and [Sefaria](https://www.sefaria.org/texts) for their own digital implementations of the Talmud.


## Support and Dependencies
This project relies on two proprietary datasets, whic prevents the code from being run unless one has access. These are:
1. Sefaria's digitized Jastrow database.
2. [Dicta](https://dicta.org.il/)'s comprehensive lexicon of Talmudic Aramaic.
The developer was generously granted access to both of these datasets.

Two open source projects were used in this project:
1. [YAP](https://github.com/onlplab/yap), a Part-of-Speech tagger for Modern Hebrew, has been used to tag the POS of words in Rabbinic Hebrew, which has a close syntactic relationship to Modern Hebrew, as the penultimate step of the data pipeline.
2. I have relied upon the aligned CAL-Sefaria Talmud data generated by Noah Santacruz for his [PSHAT](https://github.com/nsantacruz/PSHAT) project. This data was aligned with the vowelized text of the Dicta Talmud in order to create a dataset of 70,000+ vowelized Aramaic words for training the language classifier.

Python module dependencies: scikitlearn, pandas, numpy, requests, joblib. (There may also be others that I have forgotten; install as necessary.)


## Data Pipeline
1. `align_and_classify.py` -- with the raw Dicta Talmud in a local directory (`data/dicta_talmud`), this downloads the correspondinig texts of each Talmudic tractate from the Sefaria API and aligns the corresponding words. It also classifies the proper "type" of each segment (henceforth, "chunk") of the Talmud as "m" for Mishna (written in a mix of Rabbinic Hebrew and Biblical Hebrew), or "g" for Gemara (written in a mix of Aramaic, Rabbinic Hebrew, and Biblical Hebrew). The program asks for user input when the words do not line up perfectly; most tractates take only a few minutes to align, with very few human decisions. The output is a json file for each tractate that substitutes each word for a word "container" that stores the word in Sefaria's version, along with the two possible spellings provided by Dicta of that word; can be found in `data/aligned_talmud`.
2. `connect_sources.py` -- this uses the Sefaria API to download pre-Talmudic sources (Bible, Mishna, Tosefta, Midrash) that are referenced by a particular chunk and store them and the aligned text itself in another json file. This part requires no human input, but takes some time depending on the length of the tractate and the number of sources it references; can be found in `data/connected_talmud`.
3. `tag_language.py` -- using the language tagger and simple heuristics (e.g. any word that appears in a linked Biblical source should be tagged as 'B'), every word in a tractate is tagged as Biblical Hebrew (B), Rabbinic Hebrew (R), or Aramaic (A). The output is another json file for each tractate, but with page numbers and linked sources gone, as these are no longer needed; can be found at `data/lang_tagged_talmud`.
4. `tag_heb_pos.py` -- utilizes YAP to tag the POS of all words that were marked as Rabbinic Hebrew in part 4. The output is another json, with every word in the Talmud having a POS tag; words not marked as 'R' are labelled 'yydot' by YAP; located at `data/pos_tagged_talmud`. This is necessary, as there is no database mapping all Hebrew words to their corresponding roots to be directly linked to the Jastrow database, unlike for Aramaic and Biblical Hebrew. Rather, as a workaround, the Hebrew translator pipes the word through the Morfix mobile API. This returns a range of context- and vowel-independent root suggestions, along with their Parts-of-Speech. Hence, knowing the probably POS of a Rabbinic Hebrew word will help wittle down and rank the options.
5. `translate_masekhet.py` -- Translates the text, linking each word in the Talmud to its proper location (RID) in the Jastrow.


## Compiling the datasets
The most essential datasets are compiled using the data and scripts found in the `database_makers` folder (not available on GitHub because they use proprietary data). `quick-jastrow` and `tag-jastrow` can be ignored, as they simply create a quickly searchable Jastrow word-to-RID dictionary and tag the Jastrow by probable language and POS (respectively). These scripts have already been run, do not need further modification, and their outputs have been exported to their proper locations. As for the other two:

1. `root-to-entry` contains independent scripts that map roots from the BDB, [Pealim](https://www.pealim.com/dict/) word tables, and Dicta Aramaic dataset to their most probable corresponding Jastrow entries. These can then be edited manually; this has been done for the entire Dicta dataset, but not the other two, as they are unqiedly and have extraneous words that necessarily will not appear in the Jastrow. Then, run the script that compiles these root-to-entry maps into pickled dictionaries.
2. `word-to-entry` has scripts that take the outputs of `root-to-entry` and links directly from every individual word in the 3 datasets to the corresponding RIDs from the previous step. The output pickle files are sent directly to the `word-maps` folder.



## Notes
Mishnayot are always tagged by the language tagger as Rabbinic or Biblical Hebrew. There are some instances where Aramaic appears in the Mishna, but since these instances are so rare, they should be properly mapped manually on a case-by-case basis. A complete list of these can be found in Strack and Stemberger, "Introduction to the Talmud and Midrash."